{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef8a2ac4",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mibarguen/recommendations-at-scale/blob/main/notebooks/week3_ann_user_representations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a424b415-b8ef-47b3-84ee-7c40332e01ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a424b415-b8ef-47b3-84ee-7c40332e01ef",
    "outputId": "5a16988b-7a49-4ccb-efc3-0e6324020197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -lotly (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -lotly (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.2-cp38-cp38-macosx_10_9_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -lotly (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: faiss-cpu\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -lotly (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed faiss-cpu-1.7.2\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -lotly (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -lotly (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -lotly (/opt/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu --no-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "837dcaa9-cc0b-4345-bbee-3a09683decf9",
   "metadata": {
    "id": "837dcaa9-cc0b-4345-bbee-3a09683decf9"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f1f7cb-9441-4fe1-962e-9e4cb3e3c192",
   "metadata": {
    "id": "41f1f7cb-9441-4fe1-962e-9e4cb3e3c192"
   },
   "source": [
    "# **Welcome to week 3 project!**\n",
    "\n",
    "Congratulations on making it to week 3! üëè In the first week of this course, we covered the basics of how to design personalized recommendation systems. We then provided some system design examples for large scale recommenders from corporations like Spotify and YouTube, as well as techniques for candidate generation, specifically the two-tower model being used at Twitter and Pinterest.\n",
    "\n",
    "Last week, we covered details of ML approaches for recommendations: including multi-task recommenders and contextual bandits.\n",
    "\n",
    "In week 3, we covered various techniques for learning user representations.\n",
    "\n",
    "In this week's project, we will touch upon two key aspects related to representations:\n",
    "1. How do we query large amount of vectors in efficient time.\n",
    "2. How can we infer various user representations and see what their impact is on downstream task.\n",
    "\n",
    "Lets begin with Part A, which tells us how we could handle a large number of candidate items or user representations in an efficient manner. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e01328-6834-4b11-85c1-03867e1d4860",
   "metadata": {
    "id": "d0e01328-6834-4b11-85c1-03867e1d4860"
   },
   "source": [
    "# Part A: Approximate nearest neighbor search\n",
    "\n",
    "Often we are interested in finding nearest neighbors in a large space of vectors. To store embeddings for 400 million users and over 100 million items and querying them in real time is a challenging task. This is where approximate nearest neighbor approaches step in to help. Annoy, Faiss, ScaNN are typical libraries that are used for efficient vector similarity search at scale. They implement algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM.\n",
    "\n",
    "In the first part of this week's project, we will simulate embeddings of 1 million items and try to find k-nearest neighbours for an item of interest. We will implement a vanilla search function to fetch the top-k nearest neighbors and estimate the time it takes for us to do so. We will then compare this with FAISS -- Facebook's nearest neighbour search library, and compare the time it takes for us to get nearest neighbours from FAISS versus our own implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa2b438-be53-4f39-b990-859046a4a560",
   "metadata": {
    "id": "3fa2b438-be53-4f39-b990-859046a4a560"
   },
   "source": [
    "Lets first generate a simulated dataset of embeddings of 1 million items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8c36fc8b-a032-4b48-bbeb-3bff46624615",
   "metadata": {
    "id": "8c36fc8b-a032-4b48-bbeb-3bff46624615"
   },
   "outputs": [],
   "source": [
    "d = 64                           # dimension\n",
    "nb = 1000000                     # database size\n",
    "nq = 10000                       # nb of queries\n",
    "np.random.seed(1234)             # make reproducible\n",
    "xb = np.random.random((nb, d)).astype('float32')\n",
    "xq = np.random.random((nq, d)).astype('float32')\n",
    "\n",
    "k=4\n",
    "query_vector = xb[2:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c00a5-f36b-4a5f-9e27-4a3e38b467d7",
   "metadata": {
    "id": "142c00a5-f36b-4a5f-9e27-4a3e38b467d7"
   },
   "source": [
    "Now that we have these items, lets take up the goal of finding the top-5 items closest to this specific item. Your goal is to implement your function to estimate the top-5 items and print the average distance of these top 5 items to the query item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "84a94ee2-3131-44c5-ba95-ec67aa675048",
   "metadata": {
    "id": "84a94ee2-3131-44c5-ba95-ec67aa675048"
   },
   "outputs": [],
   "source": [
    "def find_top_k_nn(query_vector, k):\n",
    "    \"\"\"\n",
    "    In this function, implement your definition of top-k nearest neighbours, and return the distances\n",
    "    and indices of the these top-k items.\n",
    "    I implemented a euclidean distance to find the top k nearest neighbors\n",
    "    \"\"\"\n",
    "    dists = np.sqrt(np.sum((query_vector - xb)**2, axis=1))\n",
    "    sorted_idx_dists = np.argsort(dists)[:k]\n",
    "    closest_dists = dists[sorted_idx_dists]\n",
    "    return closest_dists, sorted_idx_dists\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a895ce-4f0b-41a5-8243-90f18bff8d7b",
   "metadata": {
    "id": "b8a895ce-4f0b-41a5-8243-90f18bff8d7b"
   },
   "source": [
    "With your top-k NN function implemented, call this function to get the top-k nearest neighbor items for the query_vector and print the average distance. Also, print the time it takes to run this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0df9e9f8-a854-4896-9ebd-fd74bad21690",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "0df9e9f8-a854-4896-9ebd-fd74bad21690",
    "outputId": "42ca8e55-76ca-4dae-f960-6b0f5cb37c23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances from the k nearest neighbor fetched: [0.        2.054384  2.0941474 2.1664221]\n",
      "indices from the k nearest neighbor fetched: [     2 379284 539651 400245]\n",
      "average distance of the k- nearest neighbors fetched:  1.5787385\n",
      "CPU times: user 242 ms, sys: 73.2 ms, total: 315 ms\n",
      "Wall time: 322 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = find_top_k_nn(query_vector, k)\n",
    "print(\"distances from the k nearest neighbor fetched:\",D)\n",
    "print(\"indices from the k nearest neighbor fetched:\",I)\n",
    "print(\"average distance of the k- nearest neighbors fetched: \",D.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4452f5-ce17-43df-95f2-e9b58e8cad5e",
   "metadata": {
    "id": "bd4452f5-ce17-43df-95f2-e9b58e8cad5e"
   },
   "source": [
    "Now lets switch to using Faiss https://github.com/facebookresearch/faiss\n",
    "\n",
    "Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6427be9-53b2-446c-851b-b2f6b7db9fa6",
   "metadata": {
    "id": "b6427be9-53b2-446c-851b-b2f6b7db9fa6"
   },
   "source": [
    "### Similarity search in Faiss\n",
    "\n",
    "Given a set of vectors x_i in dimension d, Faiss builds a data structure in RAM. After the structure is constructed, when given a new vector x in dimension d it performs efficiently the operation:\n",
    "\n",
    "$i = argmin_i ||x - x_i||$\n",
    "\n",
    "where ||.|| is the Euclidean distance (L2).\n",
    "\n",
    "In Faiss terms, the data structure is an index, an object that has an add method to add x_i vectors. Note that the x_i's are assumed to be fixed. Computing the argmin is the search operation on the index.\n",
    "\n",
    "### Indexes used by Faiss\n",
    "\n",
    "1. The inverted file from ‚ÄúVideo google: A text retrieval approach to object matching in videos.‚Äù, Sivic & Zisserman, ICCV 2003. This is the key to non-exhaustive search in large datasets. Otherwise all searches would need to scan all elements in the index, which is prohibitive even if the operation to apply for each element is fast\n",
    "\n",
    "\n",
    "2. The product quantization (PQ) method from ‚ÄúProduct quantization for nearest neighbor search‚Äù, J√©gou & al., PAMI 2011. This can be seen as a lossy compression technique for high-dimensional vectors, that allows relatively accurate reconstructions and distance computations in the compressed domain.\n",
    "\n",
    "\n",
    "3. The three-level quantization (IVFADC-R aka IndexIVFPQR) method from \"Searching in one billion vectors: re-rank with source coding\", Tavenard & al., ICASSP'11."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e63810d-fd22-49ca-b4bd-423ebf348c5f",
   "metadata": {
    "id": "3e63810d-fd22-49ca-b4bd-423ebf348c5f"
   },
   "source": [
    "We will implement these three indexes from faiss and use each of these three to search the index, and get the top-k nearest neighbour vectors, and estimate the average distance.\n",
    "\n",
    "Lets first construct the three indexes: index1, index2, index3 based on Flat index, Inverted index and product quantization techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76487ec4-a020-4ccd-aa33-f6ef25869048",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76487ec4-a020-4ccd-aa33-f6ef25869048",
    "outputId": "8f8f947f-c32e-4a5b-92db-0fbaecbe542d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of vectors indexed =  1000000\n",
      "CPU times: user 87.5 ms, sys: 61.4 ms, total: 149 ms\n",
      "Wall time: 148 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "index1 = faiss.IndexFlatL2(d)   # build the index\n",
    "index1.add(xb)                  # add vectors to the index\n",
    "print(\"total number of vectors indexed = \",index1.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55447035-57e2-4938-837b-9bf20bf66ff6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55447035-57e2-4938-837b-9bf20bf66ff6",
    "outputId": "269f5ed4-2448-4bbc-af47-8161845d0b5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of vectors indexed =  1000000\n",
      "CPU times: user 2.08 s, sys: 476 ms, total: 2.55 s\n",
      "Wall time: 418 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlist = 100\n",
    "quantizer = faiss.IndexFlatL2(d)  # the other index\n",
    "index2 = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)\n",
    "index2.train(xb)\n",
    "index2.add(xb)\n",
    "print(\"total number of vectors indexed = \",index2.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f04ebe3c-705d-49b2-aa7b-3602c111c754",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f04ebe3c-705d-49b2-aa7b-3602c111c754",
    "outputId": "ee5d4bbf-6a57-4f55-b855-3936da5a099f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of vectors indexed =  1000000\n",
      "CPU times: user 28.5 s, sys: 1.25 s, total: 29.7 s\n",
      "Wall time: 4.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlist = 100\n",
    "m = 8\n",
    "k = 4\n",
    "quantizer = faiss.IndexFlatL2(d)  # this remains the same\n",
    "index3 = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)\n",
    "                                  # 8 specifies that each sub-vector is encoded as 8 bits\n",
    "index3.train(xb)\n",
    "index3.add(xb)\n",
    "print(\"total number of vectors indexed = \",index3.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a6682-f771-4672-8998-307f4cfad5f5",
   "metadata": {
    "id": "047a6682-f771-4672-8998-307f4cfad5f5"
   },
   "source": [
    "Now that we have these three indexes, let us query these to fetch the top-k nearest neghbour for our query_vector and compute the average distance we obtain for each.\n",
    "\n",
    "We will also time these commands, to find out the trade-off between accuracy and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e54dc7d-5d15-47b2-85e4-c583f92d52a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2e54dc7d-5d15-47b2-85e4-c583f92d52a8",
    "outputId": "5052a190-c81e-43d9-854c-84cdb824961d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances from the k nearest neighbor fetched: [[0.        4.2204943 4.385453  4.693385 ]]\n",
      "indices from the k nearest neighbor fetched: [[     2 379284 539651 400245]]\n",
      "average distance of the k- nearest neighbors fetched:  3.3248332\n",
      "CPU times: user 25.6 ms, sys: 2.61 ms, total: 28.2 ms\n",
      "Wall time: 25.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = index1.search(query_vector, k)\n",
    "print(\"distances from the k nearest neighbor fetched:\", D)\n",
    "print(\"indices from the k nearest neighbor fetched:\", I)\n",
    "print(\"average distance of the k- nearest neighbors fetched: \", D.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be8f1b17-6108-41fd-814c-caa996fb731d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be8f1b17-6108-41fd-814c-caa996fb731d",
    "outputId": "2e0ecbda-dd59-4e3e-db7c-6ae4a465ae74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances from the k nearest neighbor fetched: [[0.        5.0635023 5.4133472 5.6424046]]\n",
      "indices from the k nearest neighbor fetched: [[     2 859123 177280  74082]]\n",
      "average distance of the k- nearest neighbors fetched:  4.029814\n",
      "CPU times: user 2.26 ms, sys: 1.68 ms, total: 3.94 ms\n",
      "Wall time: 2.22 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = index2.search(query_vector, k)\n",
    "print(\"distances from the k nearest neighbor fetched:\", D)\n",
    "print(\"indices from the k nearest neighbor fetched:\", I)\n",
    "print(\"average distance of the k- nearest neighbors fetched: \", D.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b230291-517c-49c5-a634-f9d4d331e241",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b230291-517c-49c5-a634-f9d4d331e241",
    "outputId": "e0b43671-bd84-412a-b9ff-49148d1805d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances from the k nearest neighbor fetched: [[1.0727246 4.9955387 5.125149  5.128112 ]]\n",
      "indices from the k nearest neighbor fetched: [[     2 351653 703885 887285]]\n",
      "average distance of the k- nearest neighbors fetched:  4.080381\n",
      "CPU times: user 2.27 ms, sys: 1.65 ms, total: 3.93 ms\n",
      "Wall time: 2.53 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = index3.search(query_vector, k)\n",
    "print(\"distances from the k nearest neighbor fetched:\", D)\n",
    "print(\"indices from the k nearest neighbor fetched:\", I)\n",
    "print(\"average distance of the k- nearest neighbors fetched: \", D.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c902e3-c9bf-408e-9052-11af5a3b069f",
   "metadata": {
    "id": "c2c902e3-c9bf-408e-9052-11af5a3b069f"
   },
   "source": [
    "Running all these, we observe that the product quantization based index is an order of magnitude faster than the inverted index. In terms of accuracy, if we assume that the lower the distance the more accurate the result, FlatIndex gives us the least distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a818f-19ab-4c26-8532-922c9aa1d673",
   "metadata": {
    "id": "044a818f-19ab-4c26-8532-922c9aa1d673"
   },
   "source": [
    "### Goal 1 for this week: Implement your k-NN function and time it\n",
    "\n",
    "The main goal for this part of the project is to implement your vanilla nearest neighbor function and fetch the closest k nearest neighbours to the query vector. Important to note that your implementation will give an exact result, i.e., your implementation will find the exact closest k vectors that will give the minimum distance to the query_vector.\n",
    "\n",
    "Please compile the results in a table, and compare the average distance obtained and the time it took to query the 1 million vectors. A nice 2D plot would also give you a good idea of the speed-accuracy trade-off involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "742e4c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>average_time</th>\n",
       "      <th>distance_1</th>\n",
       "      <th>distance_2</th>\n",
       "      <th>distance_3</th>\n",
       "      <th>distance_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>index_1</td>\n",
       "      <td>25.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.22</td>\n",
       "      <td>4.38</td>\n",
       "      <td>4.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>index_2</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.06</td>\n",
       "      <td>5.41</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>index_3</td>\n",
       "      <td>2.53</td>\n",
       "      <td>1.07</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.12</td>\n",
       "      <td>5.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exact_search</td>\n",
       "      <td>322.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2.09</td>\n",
       "      <td>2.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         method  average_time  distance_1  distance_2  distance_3  distance_4\n",
       "0       index_1         25.40        0.00        4.22        4.38        4.69\n",
       "1       index_2          2.26        0.00        5.06        5.41        5.64\n",
       "2       index_3          2.53        1.07        5.00        5.12        5.12\n",
       "3  exact_search        322.00        0.00        2.05        2.09        2.17"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_1 = {'method': 'index_1','average_time': 25.4, 'distance_1': 0, 'distance_2': 4.22, 'distance_3': 4.38, 'distance_4': 4.69}\n",
    "index_2 = {'method': 'index_2', 'average_time': 2.26, 'distance_1': 0, 'distance_2': 5.06, 'distance_3': 5.41, 'distance_4': 5.64}\n",
    "index_3 = {'method': 'index_3', 'average_time': 2.53, 'distance_1': 1.07, 'distance_2': 5, 'distance_3': 5.12, 'distance_4': 5.12}\n",
    "exact_search = {'method': 'exact_search', 'average_time': 322, 'distance_1': 0, 'distance_2': 2.05, 'distance_3': 2.09, 'distance_4': 2.17}\n",
    "\n",
    "k_nearest_comparison_df = pd.DataFrame.from_dict([index_1, index_2, index_3, exact_search],)\n",
    "k_nearest_comparison_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db46c20c",
   "metadata": {},
   "source": [
    "---\n",
    "distances from the k nearest neighbor fetched: [[0.        4.2204943 4.385453  4.693385 ]]\n",
    "indices from the k nearest neighbor fetched: [[     2 379284 539651 400245]]\n",
    "average distance of the k- nearest neighbors fetched:  3.3248332\n",
    "CPU times: user 25.6 ms, sys: 2.61 ms, total: 28.2 ms\n",
    "Wall time: 25.4 ms\n",
    "\n",
    "\n",
    "distances from the k nearest neighbor fetched: [[0.        5.0635023 5.4133472 5.6424046]]\n",
    "indices from the k nearest neighbor fetched: [[     2 859123 177280  74082]]\n",
    "average distance of the k- nearest neighbors fetched:  4.029814\n",
    "CPU times: user 2.26 ms, sys: 1.68 ms, total: 3.94 ms\n",
    "Wall time: 2.22 ms\n",
    "\n",
    "\n",
    "distances from the k nearest neighbor fetched: [[1.0727246 4.9955387 5.125149  5.128112 ]]\n",
    "indices from the k nearest neighbor fetched: [[     2 351653 703885 887285]]\n",
    "average distance of the k- nearest neighbors fetched:  4.080381\n",
    "CPU times: user 2.27 ms, sys: 1.65 ms, total: 3.93 ms\n",
    "Wall time: 2.53 ms\n",
    "\n",
    "\n",
    "distances from the k nearest neighbor fetched: [     2 379284 539651 400245]\n",
    "indices from the k nearest neighbor fetched: [0.        2.054384  2.0941474 2.1664221]\n",
    "average distance of the k- nearest neighbors fetched:  329795.5\n",
    "CPU times: user 217 ms, sys: 61.3 ms, total: 279 ms\n",
    "Wall time: 279 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e69dd09-6a21-49a8-8ca8-4d5164a7e674",
   "metadata": {
    "id": "2e69dd09-6a21-49a8-8ca8-4d5164a7e674"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e127987e-44f9-43b3-9219-1036edd0d14c",
   "metadata": {
    "id": "e127987e-44f9-43b3-9219-1036edd0d14c"
   },
   "source": [
    "# Part B: User representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c18e33-3a62-4afe-95af-9d33ef87917c",
   "metadata": {
    "id": "13c18e33-3a62-4afe-95af-9d33ef87917c"
   },
   "source": [
    "In the second part of this week's project, we wish to understand few ways of estimating user representations, and how it impacts the performance of downstream tasks.\n",
    "\n",
    "To this end, we will work on top of our H&M dataset, and develop a few different ways of representing users.\n",
    "\n",
    "The broader framework here will be -- we fix the article representations, and fix the downstream task, and then vary the user representations and see how the performance of the downstream task changes based on different user representation techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251abdd1-1cd4-41f9-af70-23d22a1aa455",
   "metadata": {
    "id": "251abdd1-1cd4-41f9-af70-23d22a1aa455"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "from contextlib import redirect_stdout\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219f3784-220e-4889-ae88-09c80f556479",
   "metadata": {
    "id": "219f3784-220e-4889-ae88-09c80f556479"
   },
   "source": [
    "While we have used neural models so far, lets try a tree based model for this task. We use LightGBM library to train the main model. Lets set up few parameters for the lightgbm model, and specify some additional parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e76b18-8d9e-4a1b-bf84-a929c8f16c6f",
   "metadata": {
    "id": "21e76b18-8d9e-4a1b-bf84-a929c8f16c6f"
   },
   "outputs": [],
   "source": [
    "rand = 64\n",
    "lgb_params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"max_depth\": -1,\n",
    "    \"num_leaves\": 40,\n",
    "    \"subsample\": 0.8,\n",
    "    \"subsample_freq\": 1,\n",
    "    \"bagging_seed\": rand,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.6,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "    \"lambda_l1\": 0,\n",
    "    \"lambda_l2\": 0,\n",
    "    \"random_state\": rand,\n",
    "    \"metric\": \"auc\",#\"binary_logloss\",\n",
    "    \"verbose\": -1\n",
    "}\n",
    "\n",
    "tran_dtypes = {\"t_dat\":\"str\",\n",
    "               \"customer_id\":\"str\",\n",
    "               \"article_id\":\"int\",\n",
    "               \"product_code\":\"int\",\n",
    "               \"price\":\"float\",\n",
    "               \"sales_channel_id\":\"int\"}\n",
    "art_dtypes = {\"article_id\":\"int\",\n",
    "              \"product_code\":\"int\",\n",
    "              \"product_type_no\":\"int\",\n",
    "              \"graphical_appearance_no\":\"int\",\n",
    "              \"colour_group_code\":\"int\",\n",
    "              \"department_no\":\"int\",\n",
    "              \"index_code\":\"str\",\n",
    "              \"index_group_no\":\"int\",\n",
    "              \"section_no\":\"int\",\n",
    "              \"garment_group_no\":\"int\"}\n",
    "cust_dtypes = {\"customer_id\":\"str\"}\n",
    "\n",
    "obj = \"class\" # \"class\" or \"rank\"\n",
    "N = 15000\n",
    "n_iter = 2 # num of iteration\n",
    "idx_file = \"exp1\"\n",
    "n_round = 2000\n",
    "n_splits = 1\n",
    "nobuy = 20 # num of negative samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df9c2e0-af9e-406d-a9c6-c38a87347728",
   "metadata": {
    "id": "3df9c2e0-af9e-406d-a9c6-c38a87347728"
   },
   "source": [
    "While we vary the user represnetations, we will keep the article representation fixed. The code below reads the article.csv file and extracts a number of features to represent articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb93a7cd-a637-4960-9577-08449a57b55d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb93a7cd-a637-4960-9577-08449a57b55d",
    "outputId": "cc893af0-cede-4576-fbc5-c001c97e7151"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/hmdata/articles.csv\")\n",
    "\n",
    "## Find categorical columns\n",
    "ohe_columns = []\n",
    "total = 0\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == \"int64\" and len(df[col].unique()) <= 500:\n",
    "        ohe_columns.append(col)\n",
    "        total += len(df[col].unique())\n",
    "        \n",
    "## Do one hot encoding of the above categorical variables\n",
    "V = pd.get_dummies(df[ohe_columns], columns=ohe_columns).values\n",
    "\n",
    "\n",
    "## Get article features\n",
    "tfidf = TfidfVectorizer(min_df=3)\n",
    "V_desc = tfidf.fit_transform(df[\"detail_desc\"].fillna(\"nodesc\"))\n",
    "\n",
    "## Represent articles as vector of size 512\n",
    "EMB_SIZE = 512\n",
    "V = np.hstack([V.astype(\"float32\"), V_desc.todense()])\n",
    "svd = TruncatedSVD(n_components=EMB_SIZE, random_state=0)\n",
    "svd.fit(V)\n",
    "V = svd.transform(V)\n",
    "\n",
    "np.save(\"articles.npy\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a3715e-de70-49d4-b057-e0b674ca65ff",
   "metadata": {
    "id": "64a3715e-de70-49d4-b057-e0b674ca65ff"
   },
   "outputs": [],
   "source": [
    "def item_representation_1():\n",
    "    df_art = pd.read_csv(\"../data/hmdata/articles.csv\",dtype=art_dtypes)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df_art[\"index_code\"].unique())\n",
    "    df_art[\"index_code\"] = le.transform(df_art[\"index_code\"])\n",
    "    \n",
    "    dict_vec = {}\n",
    "    vec_art = np.load(\"articles.npy\")\n",
    "    df_vec = pd.concat([df_art[\"article_id\"],pd.DataFrame(vec_art)],axis=1)\n",
    "    for i in range(len(vec_art)):\n",
    "        dict_vec[df_art[\"article_id\"][i]] = vec_art[i]\n",
    "    del vec_art,df_vec\n",
    "    \n",
    "    return df_art, dict_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbe5f64-354c-451e-a9e0-eb114440f8f0",
   "metadata": {
    "id": "7cbe5f64-354c-451e-a9e0-eb114440f8f0"
   },
   "source": [
    "Taken together, the two cells above give us all the features we want to represent articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c3082e-1da7-466f-8b9f-c777c8ef63cd",
   "metadata": {
    "id": "f0c3082e-1da7-466f-8b9f-c777c8ef63cd"
   },
   "source": [
    "Now lets define some functions to extract user representations. The different functions will contain different ways of representing users.\n",
    "\n",
    "We bootstrap by providing a simple set of features to represent users in user_representation_1(). This function returns the dataframe of user features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de63a95-ef6c-4796-8e59-0cb33b9b948e",
   "metadata": {
    "id": "5de63a95-ef6c-4796-8e59-0cb33b9b948e"
   },
   "outputs": [],
   "source": [
    "def user_representation_1():\n",
    "    df_cust = pd.read_csv(\"../data/hmdata/customers.csv\",dtype=cust_dtypes)\n",
    "    df_cust[\"age\"] = df_cust[\"age\"].fillna(df_cust[\"age\"].mean())\n",
    "    df_cust[[\"FN\",\"Active\"]] = df_cust[[\"FN\",\"Active\"]].fillna(0)\n",
    "    df_cust[\"club_member_status\"] = df_cust[\"club_member_status\"].apply(lambda x:1 if x == \"ACTIVE\" else 0)\n",
    "    df_cust[\"fashion_news_frequency\"] = df_cust[\"fashion_news_frequency\"].apply(lambda x:0 if x == \"NONE\" else 1)\n",
    "    df_cust = df_cust.drop([\"postal_code\"], axis=1)\n",
    "    return df_cust\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65dbcef5-701a-4999-b696-3d871ccb485d",
   "metadata": {
    "id": "65dbcef5-701a-4999-b696-3d871ccb485d"
   },
   "outputs": [],
   "source": [
    "def user_representation_2():\n",
    "    \"\"\"\n",
    "    TODO -- compute user representations as the average\n",
    "    of the embeddings of the recently purchased articles\n",
    "    return user representation\n",
    "\n",
    "    Hint: You may find pd.DataFrame(item_representation_1()[1]).transpose() useful\n",
    "    \"\"\"\n",
    "    transactions_df = pd.read_csv('../data/hmdata/transactions_train.csv',\n",
    "                             usecols=['article_id', 'customer_id'], \n",
    "                             nrows=70000)\n",
    "    \n",
    "    embeddings_df = pd.DataFrame(item_representation_1()[1]).transpose()\n",
    "    embeddings_df = embeddings_df.reset_index()\n",
    "    embeddings_df.rename({'index': 'article_id'}, axis=1, inplace=True)\n",
    "    \n",
    "    transactions_df_embeddings = pd.merge(transactions_df, embeddings_df, \n",
    "                                     on='article_id', how='left')\n",
    "    \n",
    "    return transactions_df_embeddings\n",
    "\n",
    "def user_representation_3():\n",
    "    \"\"\"\n",
    "    OPTIONAL -- compute user representations as the output\n",
    "    of the doc2vec model.\n",
    "    https://cs.stanford.edu/~quocle/paragraph_vector.pdf\n",
    "    Doc2vec model is an embedding learning method\n",
    "    that enables us to learn representations of a document.\n",
    "    We treat each user as a document, and the set of articles\n",
    "    the user has purchased as the set of words in the document.\n",
    "    \"\"\"\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b7430-bf43-4f16-b7a8-a6edbd078e1d",
   "metadata": {
    "id": "d39b7430-bf43-4f16-b7a8-a6edbd078e1d"
   },
   "source": [
    "As part of the goal for part B of this week's project, please use the above two functions to implement the two user representation techniques mentioned in the project jumpstart.\n",
    "\n",
    "You can run the rest of the notebook for now, and come back to these functions, implement them and re-run some of the code below and use user_representation_2() (and optionally user_representation_3()) to get the appropriate user features to use to train the model for the downstream task.\n",
    "\n",
    "Lets write a function that would read the transactions data and return the dataframes for the transactions within the dates we want to consider, along with the dataframes for articles features: df_art and dict_vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "55c3e4dd-ce84-4199-9215-06490305db3c",
   "metadata": {
    "id": "55c3e4dd-ce84-4199-9215-06490305db3c"
   },
   "outputs": [],
   "source": [
    "path = \"../data/hmdata/\"\n",
    "def read_data(day_oldest):\n",
    "    df_trans = pd.read_csv(path+\"transactions_train.csv\",dtype=tran_dtypes, nrows=70000)\n",
    "    df_trans[\"t_dat\"] = pd.to_datetime(df_trans[\"t_dat\"],format=\"%Y-%m-%d\")\n",
    "\n",
    "    df_trans = df_trans.drop_duplicates([\"customer_id\",\"article_id\",\"t_dat\"])\n",
    "    df_art,dict_vec = item_representation_1()\n",
    "    df_trans = df_trans.merge(df_art[[\"article_id\",\"product_code\",\"product_type_no\",\"graphical_appearance_no\",\"colour_group_code\",\"department_no\",\"index_code\",\"index_group_no\",\"section_no\",\"garment_group_no\"]],how=\"left\",on=\"article_id\")\n",
    "\n",
    "    return df_trans, df_art, dict_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4128aa41-0a7c-405c-a2c2-02a3e19678f2",
   "metadata": {
    "id": "4128aa41-0a7c-405c-a2c2-02a3e19678f2"
   },
   "source": [
    "Now we have all the ingredients we need -- we have a basic version of user representations and we have the article representations, and transactions data on which we can train our downstream task.\n",
    "\n",
    "The downstream task we consider is the task of predicting whether or not a user will purchase an article. This is the same task that we have been dealing with in the past 2 weeks.\n",
    "\n",
    "Lets define a train() function that will consider the start and end dates and split data based on these, generate the training data, do random negative sampling and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e0c7a4f-8e4d-42f7-b379-1ebc870375d9",
   "metadata": {
    "id": "5e0c7a4f-8e4d-42f7-b379-1ebc870375d9"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    #### Transaction start date say it is from 2019/9/23 and say we take 1 week data\n",
    "    day_start = datetime.datetime(2019,9,20)\n",
    "    #### Transaction end date\n",
    "    day_end = datetime.datetime(2019,9,21)\n",
    "    \n",
    "    ######## Splitting data based on date ###########################\n",
    "    ####### Train date ###########################################\n",
    "    \n",
    "    df_trans, df_art, dict_vec = read_data(day_oldest = datetime.datetime(2018,9,19))\n",
    "\n",
    "    df_cust = user_representation_2()\n",
    "    \n",
    "    print('---here--')\n",
    "\n",
    "    top_art_all = df_trans.groupby(\"article_id\")[\"t_dat\"].count().sort_values(ascending = False).index[:N].tolist()\n",
    "\n",
    "    \n",
    "    ############### Create training data #################################################################################\n",
    "    \n",
    "    \n",
    "    list_df_buy = []\n",
    "    list_cust =[]\n",
    "    \n",
    "    # make positive samples\n",
    "    list_df_buy = df_trans.query(f\"(article_id in @top_art_all)\").drop_duplicates([\"customer_id\",\"article_id\"])[[\"customer_id\",\"article_id\"]].copy()\n",
    "    list_df_buy[\"target\"] = 1\n",
    "    list_cust = list_df_buy[\"customer_id\"].unique().tolist()\n",
    "    \n",
    "    # make negative samples (random selection)\n",
    "    \n",
    "    list_df_nobuy = pd.concat([pd.DataFrame({\"customer_id\":x,\"article_id\":random.sample(top_art_all,nobuy)}) for x in list_cust])\n",
    "    list_df_nobuy[\"target\"] = 0\n",
    "    list_train = pd.concat([list_df_buy,list_df_nobuy]).drop_duplicates([\"customer_id\",\"article_id\"])\n",
    "    del list_df_nobuy\n",
    "\n",
    "    # add feature\n",
    "    df_train = pd.DataFrame()\n",
    "    \n",
    "    ########## Merging item features with the transactions data ###################################################\n",
    "    list_train = list_train.merge(df_art[[\"article_id\",\"product_code\",\"product_type_no\",\"graphical_appearance_no\",\"colour_group_code\",\"department_no\",\"index_code\",\"index_group_no\",\"section_no\",\"garment_group_no\"]],how=\"left\",on=\"article_id\")\n",
    "    \n",
    "    ######### Merging customer data with the above data ######################################\n",
    "    list_train = list_train.merge(df_cust, how=\"left\", on=\"customer_id\")\n",
    "    df_train = df_train.append(list_train)\n",
    "    del list_train\n",
    "    gc.collect()\n",
    "    \n",
    "    # now that we have all the data in place, lets train the lgbm model\n",
    "\n",
    "    # train lgbm\n",
    "    X_train = df_train.drop([\"customer_id\",\"product_code\",\"product_type_no\",\"department_no\",\"target\"],axis=1)\n",
    "    y_train = df_train[\"target\"]\n",
    "    del df_train\n",
    "    print('---here--6')\n",
    "    \n",
    "    X_tr, X_va, y_tr, y_va = train_test_split(X_train,y_train,stratify = y_train)\n",
    "    d_tr = lgb.Dataset(X_tr, label=y_tr,  free_raw_data=False)\n",
    "    d_va = lgb.Dataset(X_va, label=y_va,  free_raw_data=False)\n",
    "    lgbm_model = lgb.train(lgb_params, train_set=d_tr, num_boost_round=n_round, valid_sets=[d_tr,d_va], verbose_eval=500, early_stopping_rounds=100)\n",
    "    print('---here--7')\n",
    "    # save model\n",
    "    pd.to_pickle(lgbm_model,\"lgbm_model.pkl\")\n",
    "    del X_train, y_train, X_tr, X_va, y_tr, y_va, d_tr, d_va\n",
    "    gc.collect()\n",
    "    del df_trans, df_art, df_cust\n",
    "    gc.collect()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "afb1b72d-7f50-4dc8-a44c-95dcf9c537d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afb1b72d-7f50-4dc8-a44c-95dcf9c537d2",
    "outputId": "842ea858-0607-4114-e485-4c36be7d2041"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---here--\n",
      "---here--6\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\ttraining's auc: 0.858305\tvalid_1's auc: 0.852708\n",
      "[1000]\ttraining's auc: 0.876689\tvalid_1's auc: 0.866143\n",
      "[1500]\ttraining's auc: 0.888768\tvalid_1's auc: 0.87406\n",
      "[2000]\ttraining's auc: 0.898615\tvalid_1's auc: 0.880164\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining's auc: 0.898615\tvalid_1's auc: 0.880164\n",
      "---here--7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b63a5-56ee-4bd2-b7b8-2aebb5299b31",
   "metadata": {
    "id": "2a4b63a5-56ee-4bd2-b7b8-2aebb5299b31"
   },
   "source": [
    "We have now trained a light gbm model using user_representation_1() function as the user representation technique. The key goals for part B of this week's project are to implement user_representation_2(), where we represent the user as the average of embeddings of their recently purchased articles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4940f48d-08e5-4130-be2c-d15622012620",
   "metadata": {
    "id": "4940f48d-08e5-4130-be2c-d15622012620"
   },
   "source": [
    "Once you have implemented the function, please note to change the line:\n",
    "\n",
    "df_cust = user_representation_1()\n",
    "\n",
    "to the appropriate function name and run re-train the model. Please report the performance numbers with each of the two user representations.\n",
    "\n",
    "This should complete the week 3 project!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ydLW7dPiuw-E",
   "metadata": {
    "id": "ydLW7dPiuw-E"
   },
   "source": [
    "### Optional task 1: training a Doc2Vec model\n",
    "\n",
    "If you want an extra challenge, you can try implementing Doc2vec representations in user_representation_3(). The Doc2vec model is an embedding learning method\n",
    "    that enables us to learn representations of a document.\n",
    "    We treat each user as a document, and the set of articles\n",
    "    the user has purchased as the set of words in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce141145-f5af-4f2d-9d94-f4bb50e6208f",
   "metadata": {
    "id": "ce141145-f5af-4f2d-9d94-f4bb50e6208f"
   },
   "source": [
    "### Optional task 2: training a sequential LSTM model\n",
    "\n",
    "Another optional task here would be to implement user_representation_4() where user representations are learnt by a sequential LSTM model. The LSTM model will need to be trained on a task -- the task itself could be the downstream task of predicting whether or not a user would purchase a given article given a sequence of previous articles. The final hidden layer of the lstm model can be used as the user representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eb3b58-062c-4c96-b3f0-2db4407ce1b3",
   "metadata": {
    "id": "88eb3b58-062c-4c96-b3f0-2db4407ce1b3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "week3-ann-user-representations.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m89"
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
